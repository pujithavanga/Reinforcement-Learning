import numpy as np
# Define the environment (matrix)
# 0 represents empty cell, -1 represents fire, 1 represents maximum reward
# Matrix dimensions: 3x4
env_matrix = np.zeros((3, 4))
env_matrix[0, 3] = -1  # Fire at (1, 4)
env_matrix[1, 3] = -1  # Fire at (2, 4)
env_matrix[2, 3] = 1   # Maximum reward at (3, 4)
# Define parameters
gamma = 0.8           # Discount factor
alpha = 0.1           # Learning rate
num_episodes = 1000   # Number of episodes
max_steps = 100       # Maximum number of steps per episode
epsilon = 0.1         # Epsilon-greedy exploration rate
# Define actions
actions = ['up', 'down', 'left', 'right']
# Initialize Q-table with zeros
Q = np.zeros((3, 4, len(actions)))
# Q-learning algorithm
for episode in range(num_episodes):
    state = (0, 0)  # Starting position
    total_reward = 0
    for step in range(max_steps):
        # Epsilon-greedy exploration
        if np.random.uniform(0, 1) < epsilon:
            action = np.random.choice(actions)
        else:
            action = actions[np.argmax(Q[state[0], state[1]])]
        # Take action and observe next state and reward
        if action == 'up':
            next_state = (max(state[0] - 1, 0), state[1])
        elif action == 'down':
            next_state = (min(state[0] + 1, 2), state[1])
        elif action == 'left':
            next_state = (state[0], max(state[1] - 1, 0))
        elif action == 'right':
            next_state = (state[0], min(state[1] + 1, 3))
        reward = env_matrix[next_state[0], next_state[1]]
        # Update Q-value using Q-learning equation
        Q[state[0], state[1], actions.index(action)] += alpha * (reward + gamma * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], actions.index(action)])
        # Move to next state
        state = next_state
        total_reward += reward
        # Check if episode is finished
        if reward == 1 or reward == -1:
            break
        # Print total reward for each episode
    print(f"Episode {episode + 1}, Total Reward: {total_reward}")
# Final Q-values
print("\nFinal Q-values:")
print(Q)
